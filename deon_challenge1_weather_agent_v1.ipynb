{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O7vTrADJlr0rEQ500XylQPCR",
      "metadata": {
        "id": "O7vTrADJlr0rEQ500XylQPCR",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ARCoeUZCRNGi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARCoeUZCRNGi",
        "outputId": "acd8e923-e0a1-4f0d-a832-c8151897d0f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.1/218.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.7/335.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for litellm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Installation complete.\n"
          ]
        }
      ],
      "source": [
        "# @title Step 0: Setup and Installation\n",
        "# Install ADK and LiteLLM for multi-model support\n",
        "\n",
        "!pip install google-adk -q\n",
        "!pip install litellm -q\n",
        "\n",
        "print(\"Installation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sbwxKypOSBkN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbwxKypOSBkN",
        "outputId": "cc2f54b3-10fa-4e55-c410-817f275f677f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ],
      "source": [
        "# @title Import necessary libraries\n",
        "import os\n",
        "import asyncio\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "print(\"Libraries imported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3mNsVI5eSDOi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mNsVI5eSDOi",
        "outputId": "b595b992-2d97-44cd-b9b9-5a205acf3500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Keys Set:\n",
            "Google API Key set: Yes\n",
            "OpenAI API Key set: Yes\n",
            "Anthropic API Key set: No (REPLACE PLACEHOLDER!)\n"
          ]
        }
      ],
      "source": [
        "# @title Configure API Keys (Replace with your actual keys!)\n",
        "\n",
        "# --- IMPORTANT: Replace placeholders with your real API keys ---\n",
        "\n",
        "\n",
        "# Gemini API Key (Get from Google AI Studio: https://aistudio.google.com/app/apikey)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\" # <--- REPLACE\n",
        "\n",
        "# [Optional]\n",
        "# OpenAI API Key (Get from OpenAI Platform: https://platform.openai.com/api-keys)\n",
        "os.environ['OPENAI_API_KEY'] = \"\" # <--- REPLACE\n",
        "\n",
        "# [Optional]\n",
        "# Anthropic API Key (Get from Anthropic Console: https://console.anthropic.com/settings/keys)\n",
        "os.environ['ANTHROPIC_API_KEY'] = 'YOUR_ANTHROPIC_API_KEY' # <--- REPLACE\n",
        "\n",
        "\n",
        "# --- Verify Keys (Optional Check) ---\n",
        "print(\"API Keys Set:\")\n",
        "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
        "print(f\"OpenAI API Key set: {'Yes' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
        "print(f\"Anthropic API Key set: {'Yes' if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
        "\n",
        "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
        "\n",
        "\n",
        "# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MI_qvZJrSJuR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI_qvZJrSJuR",
        "outputId": "eaccb8aa-a57c-4691-91b3-54f3cd4092df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Environment configured.\n"
          ]
        }
      ],
      "source": [
        "# --- Define Model Constants for easier use ---\n",
        "\n",
        "# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
        "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
        "\n",
        "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\n",
        "MODEL_GPT_4O = \"openai/gpt-4.1\" # You can also try: gpt-4.1-mini, gpt-4o etc.\n",
        "\n",
        "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\n",
        "MODEL_CLAUDE_SONNET = \"anthropic/claude-sonnet-4-20250514\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\n",
        "\n",
        "print(\"\\nEnvironment configured.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ILy7YTCbSRAT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILy7YTCbSRAT",
        "outputId": "e1bf4ca2-7f60-44e7-c504-663516a620b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Tool: get_weather called for city: New York ---\n",
            "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}\n",
            "--- Tool: get_weather called for city: Paris ---\n",
            "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
          ]
        }
      ],
      "source": [
        "# @title Define the get_weather Tool\n",
        "def get_weather(city: str) -> dict:\n",
        "    \"\"\"Retrieves the current weather report for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the weather information.\n",
        "              Includes a 'status' key ('success' or 'error').\n",
        "              If 'success', includes a 'report' key with weather details.\n",
        "              If 'error', includes an 'error_message' key.\n",
        "    \"\"\"\n",
        "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
        "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
        "\n",
        "    # Mock weather data\n",
        "    mock_weather_db = {\n",
        "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\n",
        "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15°C.\"},\n",
        "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\n",
        "    }\n",
        "\n",
        "    if city_normalized in mock_weather_db:\n",
        "        return mock_weather_db[city_normalized]\n",
        "    else:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
        "\n",
        "# Example tool usage (optional test)\n",
        "print(get_weather(\"New York\"))\n",
        "print(get_weather(\"Paris\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Ho1COmKSUeV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ho1COmKSUeV",
        "outputId": "77d98a49-1bce-4bfc-ebb9-a2883fc39b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash'.\n"
          ]
        }
      ],
      "source": [
        "# @title Define the Weather Agent\n",
        "# Use one of the model constants defined earlier\n",
        "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"weather_agent_v1\",\n",
        "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
        "    description=\"Provides weather information for specific cities.\",\n",
        "    instruction=\"You are a helpful weather assistant. \"\n",
        "                \"When the user asks for the weather in a specific city, \"\n",
        "                \"use the 'get_weather' tool to find the information. \"\n",
        "                \"If the tool returns an error, inform the user politely. \"\n",
        "                \"If the tool is successful, present the weather report clearly.\",\n",
        "    tools=[get_weather], # Pass the function directly\n",
        ")\n",
        "\n",
        "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h30dNtqMSah5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h30dNtqMSah5",
        "outputId": "5272af0b-723e-40fb-a2cb-40eb6a1d8b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
            "Runner created for agent 'weather_agent_v1'.\n"
          ]
        }
      ],
      "source": [
        "# @title Setup Session Service and Runner\n",
        "\n",
        "# --- Session Management ---\n",
        "# Key Concept: SessionService stores conversation history & state.\n",
        "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Define constants for identifying the interaction context\n",
        "APP_NAME = \"weather_tutorial_app\"\n",
        "USER_ID = \"user_1\"\n",
        "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
        "\n",
        "# Create the specific session where the conversation will happen\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME,\n",
        "    user_id=USER_ID,\n",
        "    session_id=SESSION_ID\n",
        ")\n",
        "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
        "\n",
        "# --- Runner ---\n",
        "# Key Concept: Runner orchestrates the agent execution loop.\n",
        "runner = Runner(\n",
        "    agent=weather_agent, # The agent we want to run\n",
        "    app_name=APP_NAME,   # Associates runs with our app\n",
        "    session_service=session_service # Uses our session manager\n",
        ")\n",
        "print(f\"Runner created for agent '{runner.agent.name}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yZJr8lbkSebH",
      "metadata": {
        "id": "yZJr8lbkSebH"
      },
      "outputs": [],
      "source": [
        "# @title Define Agent Interaction Function\n",
        "\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "async def call_agent_async(query: str, runner, user_id, session_id):\n",
        "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
        "  print(f\"\\n>>> User Query: {query}\")\n",
        "\n",
        "  # Prepare the user's message in ADK format\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "\n",
        "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
        "\n",
        "  # Key Concept: run_async executes the agent logic and yields Events.\n",
        "  # We iterate through events to find the final answer.\n",
        "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
        "      # You can uncomment the line below to see *all* events during execution\n",
        "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
        "\n",
        "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
        "      if event.is_final_response():\n",
        "          if event.content and event.content.parts:\n",
        "             # Assuming text response in the first part\n",
        "             final_response_text = event.content.parts[0].text\n",
        "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
        "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
        "          # Add more checks here if needed (e.g., specific error codes)\n",
        "          break # Stop processing events once the final response is found\n",
        "\n",
        "  print(f\"<<< Agent Response: {final_response_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mEd2QhHyUKY8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEd2QhHyUKY8",
        "outputId": "54acaebc-d9d9-403c-bad7-47be5ca9ee2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> User Query: What is the weather like in London?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Tool: get_weather called for city: London ---\n",
            "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
            "\n",
            "\n",
            ">>> User Query: How about Paris?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Tool: get_weather called for city: Paris ---\n",
            "<<< Agent Response: I am sorry, I don't have weather information for Paris.\n",
            "\n",
            "\n",
            ">>> User Query: Tell me the weather in New York\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Tool: get_weather called for city: New York ---\n",
            "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Run the Initial Conversation\n",
        "\n",
        "# We need an async function to await our interaction helper\n",
        "async def run_conversation():\n",
        "    await call_agent_async(\"What is the weather like in London?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "\n",
        "    await call_agent_async(\"How about Paris?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
        "\n",
        "    await call_agent_async(\"Tell me the weather in New York\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "\n",
        "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
        "await run_conversation()\n",
        "\n",
        "# --- OR ---\n",
        "\n",
        "# Uncomment the following lines if running as a standard Python script (.py file):\n",
        "# import asyncio\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         asyncio.run(run_conversation())\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qc7dHr4ZVM6X",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc7dHr4ZVM6X",
        "outputId": "5a39d7c0-6713-4539-f4c3-781df83ade0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greeting and Farewell tools defined.\n",
            "--- Tool: say_hello called with name: Alice ---\n",
            "Hello, Alice!\n",
            "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
            "Hello there!\n",
            "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
            "Hello there!\n"
          ]
        }
      ],
      "source": [
        "# @title Define Tools for Greeting and Farewell Agents\n",
        "from typing import Optional # Make sure to import Optional\n",
        "\n",
        "# Ensure 'get_weather' from Step 1 is available if running this step independently.\n",
        "# def get_weather(city: str) -> dict: ... (from Step 1)\n",
        "\n",
        "def say_hello(name: Optional[str] = None) -> str: # MODIFIED SIGNATURE\n",
        "    \"\"\"Provides a simple greeting. If a name is provided, it will be used.\n",
        "\n",
        "    Args:\n",
        "        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.\n",
        "\n",
        "    Returns:\n",
        "        str: A friendly greeting message.\n",
        "    \"\"\"\n",
        "    # MODIFICATION START\n",
        "    if name:\n",
        "        greeting = f\"Hello, {name}!\"\n",
        "        print(f\"--- Tool: say_hello called with name: {name} ---\")\n",
        "    else:\n",
        "        greeting = \"Hello there!\" # Default greeting if name is None or not explicitly passed\n",
        "        print(f\"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---\")\n",
        "    return greeting\n",
        "    # MODIFICATION END\n",
        "\n",
        "def say_goodbye() -> str:\n",
        "    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\n",
        "    print(f\"--- Tool: say_goodbye called ---\")\n",
        "    return \"Goodbye! Have a great day.\"\n",
        "\n",
        "print(\"Greeting and Farewell tools defined.\")\n",
        "\n",
        "# Optional self-test\n",
        "print(say_hello(\"Alice\"))\n",
        "print(say_hello()) # Test with no argument (should use default \"Hello there!\")\n",
        "print(say_hello(name=None)) # Test with name explicitly as None (should use default \"Hello there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0rSLxDkx7m7y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "0rSLxDkx7m7y",
        "outputId": "d88ba75f-f9f3-4af8-c442-387c1091cc32"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-30-841988558.py, line 10)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-30-841988558.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    tools=[get_extended_weather_forecast, get_lat_lon]\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "weather_agent = Agent(\n",
        " name=\"Pat\",\n",
        " model=\"gemini-2.0-flash\",\n",
        " description=(\n",
        " \"Pat the Friendly Weather Agent.\"\n",
        " ),\n",
        " instruction=(\n",
        "WEATHER_AGENT_INSTRUCTIONS,\n",
        "),\n",
        "tools=[get_extended_weather_forecast, get_lat_lon]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pw-LfyLO8UKW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw-LfyLO8UKW",
        "outputId": "052fd5b5-4b07-4d4d-adf9-130f28e3867d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Tool: get_extended_weather_forecast called for city: New York, days: 5 ---\n",
            "{'status': 'success', 'forecast': ['Day 1: Sunny, 25°C', 'Day 2: Partly cloudy, 27°C', 'Day 3: Chance of rain, 24°C']}\n",
            "--- Tool: get_lat_lon called for city: Paris ---\n",
            "{'status': 'error', 'error_message': \"Sorry, I don't have coordinates for 'Paris' (USA cities only).\"}\n"
          ]
        }
      ],
      "source": [
        "# @title Define get_extended_weather_forecast and get_lat_lon Tools (USA Only)\n",
        "\n",
        "def get_extended_weather_forecast(city: str, days: int = 3) -> dict:\n",
        "    \"\"\"Retrieves an extended weather forecast for a specified city in the USA.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city.\n",
        "        days (int): The number of days for the forecast (default: 3).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the forecast information.\n",
        "              Includes a 'status' key ('success' or 'error').\n",
        "              If 'success', includes a 'forecast' key with weather details.\n",
        "              If 'error', includes an 'error_message' key.\n",
        "    \"\"\"\n",
        "    print(f\"--- Tool: get_extended_weather_forecast called for city: {city}, days: {days} ---\")\n",
        "    city_normalized = city.lower().replace(\" \", \"\")\n",
        "\n",
        "    # Mock forecast data (USA cities only)\n",
        "    mock_forecast_db = {\n",
        "        \"newyork\": {\"status\": \"success\", \"forecast\": [\n",
        "            \"Day 1: Sunny, 25°C\",\n",
        "            \"Day 2: Partly cloudy, 27°C\",\n",
        "            \"Day 3: Chance of rain, 24°C\"\n",
        "        ]},\n",
        "        \"losangeles\": {\"status\": \"success\", \"forecast\": [\n",
        "            \"Day 1: Sunny, 28°C\",\n",
        "            \"Day 2: Sunny, 30°C\",\n",
        "            \"Day 3: Partly cloudy, 29°C\"\n",
        "        ]},\n",
        "        \"chicago\": {\"status\": \"success\", \"forecast\": [\n",
        "            \"Day 1: Cloudy, 18°C\",\n",
        "            \"Day 2: Rain, 16°C\",\n",
        "            \"Day 3: Partly cloudy, 20°C\"\n",
        "        ]}\n",
        "    }\n",
        "\n",
        "    if city_normalized in mock_forecast_db:\n",
        "        return mock_forecast_db[city_normalized]\n",
        "    else:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have a forecast for '{city}' (USA cities only).\"}\n",
        "\n",
        "\n",
        "def get_lat_lon(city: str) -> dict:\n",
        "    \"\"\"Retrieves the latitude and longitude coordinates for a specified city in the USA.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the coordinates.\n",
        "              Includes a 'status' key ('success' or 'error').\n",
        "              If 'success', includes 'latitude' and 'longitude' keys.\n",
        "              If 'error', includes an 'error_message' key.\n",
        "    \"\"\"\n",
        "    print(f\"--- Tool: get_lat_lon called for city: {city} ---\")\n",
        "    city_normalized = city.lower().replace(\" \", \"\")\n",
        "\n",
        "    # Mock coordinates data (USA cities only)\n",
        "    mock_coordinates_db = {\n",
        "        \"newyork\": {\"status\": \"success\", \"latitude\": 40.7128, \"longitude\": -74.0060},\n",
        "        \"losangeles\": {\"status\": \"success\", \"latitude\": 34.0522, \"longitude\": -118.2437},\n",
        "        \"chicago\": {\"status\": \"success\", \"latitude\": 41.8781, \"longitude\": -87.6298}\n",
        "    }\n",
        "\n",
        "    if city_normalized in mock_coordinates_db:\n",
        "        return mock_coordinates_db[city_normalized]\n",
        "    else:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have coordinates for '{city}' (USA cities only).\"}\n",
        "\n",
        "# Example tool usage (optional test)\n",
        "print(get_extended_weather_forecast(\"New York\", days=5))\n",
        "print(get_lat_lon(\"Paris\")) # This will now return an error\n",
        "# @title Define WEATHER_AGENT_INSTRUCTIONS\n",
        "WEATHER_AGENT_INSTRUCTIONS = (\n",
        "    \"You are Pat, a friendly weather agent specializing in cities within the USA. \"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "deon-challenge1- weather agent",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
